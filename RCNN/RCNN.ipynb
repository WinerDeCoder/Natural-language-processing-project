{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK6Gp9sKxM3U",
        "outputId": "d9524d1c-6df8-41d8-b761-43551ae25e05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRIs1xe4xp4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128cc373-1a62-47c7-bb23-5aae419ec7e1"
      },
      "source": [
        "!pip install pyvi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.2.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.9 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGnLGj-vwU9t"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwxqnqHx3Na"
      },
      "source": [
        "#!cp -r \"/content/drive/MyDrive/Tài liệu HCMUT/NLP Modern/W2V+[LSTM, CRNN] Sentiment Analysis\" /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yKo_OxxyAHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0081efe6-1edd-4261-aa68-65684e35db8e"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab6-W2V+[LSTM, CRNN] Sentiment Analysis\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab6-W2V+[LSTM, CRNN] Sentiment Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z7z2szHRwU9z"
      },
      "source": [
        "data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7U6TJVNwU90",
        "outputId": "d491af77-ce61-471e-bb83-cc0f7221b47b"
      },
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)\n",
        "print(type(data_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part_test = data_test[:400]\n",
        "# #data_train += part_test\n",
        "# data_test = data_test[400:]\n",
        "# print(part_test.shape)"
      ],
      "metadata": {
        "id": "U8YvlR_S0d39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_train = pd.concat([data_train,part_test])\n",
        "# print(data_train.shape)"
      ],
      "metadata": {
        "id": "nwPV2df01DhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prV4XxvywU90"
      },
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQZymxX0wU91"
      },
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34J9F70iwU91"
      },
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uevU-I5wU92"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvZBk6kfwU93"
      },
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAfN337wU93"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXI-fxLwU94"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wAXC5HrwU94"
      },
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n7P5xlZwU94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d1feed-8c80-4122-eb50-3f6c701155f0"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFwY64D7wU95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0f3165-2edb-4e71-d9d9-740d7361ba9c"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab5-CNN+W2V_Sentiment Analysis/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "print(\"vocab size\", vocabulary_size)\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF5jaU0-wU95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e6bfaf-96a4-4505-ff6e-d4e4604e91fc"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [1,2,3,4]\n",
        "num_filters = 512\n",
        "drop = 0.46\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "################## LSTM ONLY ###############################\n",
        "#reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "################# SINGLE LSTM ####################\n",
        "#lstm_0 = LSTM(512)(reshape)\n",
        "\n",
        "# YOU W# ANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(128, return_sequences=True)(reshape)\n",
        "# lstm_1 = LSTM(256, return_sequences=True)(lstm_2)\n",
        "# lstm_3 = LSTM(512, return_sequences=True)(lstm_1)\n",
        "#lstm_0 = LSTM(256)(lstm_3)\n",
        "\n",
        "############################################################\n",
        "\n",
        "################## LSTM -> FC -> CNN ########################\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# lstm_3 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_0 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_1 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_2 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_4 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_5 = LSTM(64, dropout=0.4)(reshape)\n",
        "# lstm_6 = LSTM(64, dropout=0.4)(reshape)\n",
        "\n",
        "# concat_lstm = concatenate([lstm_0,lstm_1,lstm_2,lstm_3,lstm_4,lstm_5,lstm_6])\n",
        "\n",
        "# reshape_emb = Reshape((448, 1))(concat_lstm)\n",
        "\n",
        "# reshape_fc = Dense(448)(reshape_emb)\n",
        "\n",
        "# conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape_fc)\n",
        "# conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape_fc)\n",
        "# conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape_fc)\n",
        "# #conv_3 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape_fc)\n",
        "\n",
        "# conv_0 = MaxPool1D(300)(conv_0)\n",
        "# conv_1 = MaxPool1D(300)(conv_1)\n",
        "# conv_2 = MaxPool1D(300)(conv_2)\n",
        "# #conv_3 = MaxPool1D(300)(conv_3)\n",
        "\n",
        "# concat = concatenate([conv_0, conv_1, conv_2], axis=1)\n",
        "\n",
        "# flatten = Flatten()(concat)\n",
        "# #dropout = Dropout(drop)(flatten)\n",
        "# output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
        "\n",
        "# output = Flatten()(output)\n",
        "\n",
        "###########################################################\n",
        "\n",
        "\n",
        "################## CRNN ####################################\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "\n",
        "conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_3 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "# concat_0 = concatenate([conv_0, conv_1, conv_2])\n",
        "\n",
        "# conv_conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(concat_0)\n",
        "# conv_conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(concat_0)\n",
        "# conv_conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(concat_0)\n",
        "\n",
        "conv_0 = MaxPool1D(300)(conv_0)\n",
        "conv_1 = MaxPool1D(300)(conv_1)\n",
        "conv_2 = MaxPool1D(300)(conv_2)\n",
        "conv_3 = MaxPool1D(300)(conv_3)\n",
        "# Reshape output to match RNN dimension\n",
        "conv_0 = Reshape((-1, num_filters))(conv_0)\n",
        "conv_1 = Reshape((-1, num_filters))(conv_1)\n",
        "conv_2 = Reshape((-1, num_filters))(conv_2)\n",
        "conv_3 = Reshape((-1, num_filters))(conv_3)\n",
        "\n",
        "concat = concatenate([conv_0, conv_1, conv_2,conv_3])\n",
        "#concat = Flatten()(concat)\n",
        "#lstm_1 = LSTM(512, return_sequences=True, dropout=0.2)(concat)\n",
        "lstm_0 = LSTM(256,dropout=0.38)(concat)\n",
        "\n",
        "#YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "################### without LSTM + CNN ###############################\n",
        "#dropout = Dropout(drop)(concat)\n",
        "dropout = Dropout(drop)(lstm_0)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "output = Flatten()(output)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = tf.keras.optimizers.legacy.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)     (None, 300, 400)             3167600   ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_15 (Reshape)        (None, 300, 400)             0         ['embedding_3[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)          (None, 300, 512)             205312    ['reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)          (None, 300, 512)             410112    ['reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)          (None, 300, 512)             614912    ['reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)          (None, 300, 512)             614912    ['reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_12 (MaxPooli  (None, 1, 512)               0         ['conv1d_12[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_13 (MaxPooli  (None, 1, 512)               0         ['conv1d_13[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_14 (MaxPooli  (None, 1, 512)               0         ['conv1d_14[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooli  (None, 1, 512)               0         ['conv1d_15[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " reshape_16 (Reshape)        (None, 1, 512)               0         ['max_pooling1d_12[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_17 (Reshape)        (None, 1, 512)               0         ['max_pooling1d_13[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_18 (Reshape)        (None, 1, 512)               0         ['max_pooling1d_14[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_19 (Reshape)        (None, 1, 512)               0         ['max_pooling1d_15[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 1, 2048)              0         ['reshape_16[0][0]',          \n",
            " )                                                                   'reshape_17[0][0]',          \n",
            "                                                                     'reshape_18[0][0]',          \n",
            "                                                                     'reshape_19[0][0]']          \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 256)                  2360320   ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 256)                  0         ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 3)                    771       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)         (None, 3)                    0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7373939 (28.13 MB)\n",
            "Trainable params: 7373939 (28.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUZo87gZGYlz"
      },
      "source": [
        "### IF YOU HAVE MODEL WEIGHT AND WANNA LOAD IT\n",
        "#model.load_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTJGHh2hwU96"
      },
      "source": [
        "#define callbacks\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "# callbacks_list = [early_stopping]\n",
        "\n",
        "# model.fit(data, labels, validation_split=0.2,\n",
        "#           epochs=1, batch_size=128, callbacks=callbacks_list, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]"
      ],
      "metadata": {
        "id": "DMl8CgaWJgkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XnGi2AOnJcZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDWU9oCBwU96"
      },
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2zBNnGJwU96"
      },
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJFkP6fwU97"
      },
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV9bR1nlwU97"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvf83C-QwU98"
      },
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zubh3jfwU98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5714f9a6-e934-4ee0-ee7b-1c0035bb5435"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,12):\n",
        "  model.fit(data, labels, validation_split=0.1,\n",
        "          epochs=1, batch_size=128, callbacks=callbacks_list, shuffle=True)\n",
        "  score = model.evaluate(data_test, labels_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glFXwNVfV9ys",
        "outputId": "6a3e939e-85e2-4b6a-d4a0-e1d0dcab0fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 [==============================] - 3s 79ms/step - loss: 0.8207 - accuracy: 0.8708 - val_loss: 1.7651 - val_accuracy: 0.4255\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2726 - accuracy: 0.6848\n",
            "36/36 [==============================] - 3s 77ms/step - loss: 0.8184 - accuracy: 0.8804 - val_loss: 1.3136 - val_accuracy: 0.6569\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2547 - accuracy: 0.7133\n",
            "36/36 [==============================] - 3s 78ms/step - loss: 0.7311 - accuracy: 0.9022 - val_loss: 2.0737 - val_accuracy: 0.3765\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3114 - accuracy: 0.7010\n",
            "36/36 [==============================] - 3s 82ms/step - loss: 0.7224 - accuracy: 0.9035 - val_loss: 2.1126 - val_accuracy: 0.3608\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3644 - accuracy: 0.6695\n",
            "36/36 [==============================] - 3s 79ms/step - loss: 0.6812 - accuracy: 0.9211 - val_loss: 2.1885 - val_accuracy: 0.3686\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4907 - accuracy: 0.6600\n",
            "36/36 [==============================] - 3s 78ms/step - loss: 0.6925 - accuracy: 0.9174 - val_loss: 1.3179 - val_accuracy: 0.6627\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.3222 - accuracy: 0.6943\n",
            "36/36 [==============================] - 3s 83ms/step - loss: 0.6769 - accuracy: 0.9264 - val_loss: 2.1994 - val_accuracy: 0.3647\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.3764 - accuracy: 0.6867\n",
            "36/36 [==============================] - 3s 78ms/step - loss: 0.6014 - accuracy: 0.9364 - val_loss: 2.2514 - val_accuracy: 0.3706\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4594 - accuracy: 0.6552\n",
            "36/36 [==============================] - 3s 77ms/step - loss: 0.5680 - accuracy: 0.9501 - val_loss: 2.8194 - val_accuracy: 0.2647\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4809 - accuracy: 0.6695\n",
            "36/36 [==============================] - 3s 77ms/step - loss: 0.5712 - accuracy: 0.9397 - val_loss: 2.4933 - val_accuracy: 0.3216\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.4683 - accuracy: 0.6705\n",
            "36/36 [==============================] - 3s 82ms/step - loss: 0.5272 - accuracy: 0.9532 - val_loss: 2.4461 - val_accuracy: 0.3922\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.4240 - accuracy: 0.6771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzDo7cKqwU98"
      },
      "source": [
        "#score = model.evaluate(data_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE6zEWGNwU99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1406898-20f2-42f2-e3b9-2cc45048fa3e"
      },
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.22\n",
            "accuracy: 68.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6K00ZjfF-9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7ca328-fc98-40f2-9396-a92d459604a1"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_SlhL45wU99"
      },
      "source": [
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgukqXFGruD"
      },
      "source": [
        "!cp model.h5 /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}