{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK6Gp9sKxM3U",
        "outputId": "5781d7ae-b7e9-4e08-a859-065a7139d8a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRIs1xe4xp4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8df283c-d984-4719-a242-d19b91f15132"
      },
      "source": [
        "!pip install pyvi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.2.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGnLGj-vwU9t"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwxqnqHx3Na"
      },
      "source": [
        "#!cp -r \"/content/drive/MyDrive/Tài liệu HCMUT/NLP Modern/W2V+[LSTM, CRNN] Sentiment Analysis\" /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yKo_OxxyAHe",
        "outputId": "e3f4acba-4d1e-4a89-fa3e-af8a75be612e"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab6-W2V+[LSTM, CRNN] Sentiment Analysis\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab6-W2V+[LSTM, CRNN] Sentiment Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z7z2szHRwU9z"
      },
      "source": [
        "data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7U6TJVNwU90",
        "outputId": "2e7452a5-2e9c-4266-8984-9c2ea4d74b0f"
      },
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prV4XxvywU90"
      },
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQZymxX0wU91"
      },
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34J9F70iwU91"
      },
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uevU-I5wU92"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvZBk6kfwU93"
      },
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAfN337wU93"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXI-fxLwU94"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wAXC5HrwU94"
      },
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n7P5xlZwU94",
        "outputId": "99e321f8-03f0-4258-eb57-b9c10f0a9c99"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFwY64D7wU95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308ce57c-adfe-4f56-9d75-62eba00a968a"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab5-CNN+W2V_Sentiment Analysis/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "print(\"vocab size\", vocabulary_size)\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF5jaU0-wU95",
        "outputId": "68ec69ea-0780-4632-b6d7-6af36afb1b8d"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "################## LSTM ONLY ###############################\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "################# SINGLE LSTM ####################\n",
        "#lstm_2 = LSTM(128, return_sequences=True, dropout = 0.2)(reshape)\n",
        "lstm_0 = LSTM(128, dropout = 0.1)(reshape)\n",
        "\n",
        "# YOU W# ANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "\n",
        "################## CRNN ####################################\n",
        "#reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "# conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "# conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "# conv_0 = MaxPool1D(300)(conv_0)\n",
        "# conv_1 = MaxPool1D(300)(conv_1)\n",
        "# conv_2 = MaxPool1D(300)(conv_2)\n",
        "# # Reshape output to match RNN dimension\n",
        "# conv_0 = Reshape((-1, num_filters))(conv_0)\n",
        "# conv_1 = Reshape((-1, num_filters))(conv_1)\n",
        "# conv_2 = Reshape((-1, num_filters))(conv_2)\n",
        "\n",
        "# concat = concatenate([conv_0, conv_1, conv_2])\n",
        "#concat = Flatten()(concat)\n",
        "#lstm_0 = LSTM(512)(concat)\n",
        "\n",
        "# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "#dropout = Dropout(drop)(concat)\n",
        "dropout = Dropout(drop)(lstm_0)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "#output = Flatten()(output)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = tf.keras.optimizers.legacy.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 300)]             0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 300, 400)          3167600   \n",
            "                                                                 \n",
            " reshape_12 (Reshape)        (None, 300, 400)          0         \n",
            "                                                                 \n",
            " lstm_27 (LSTM)              (None, 128)               270848    \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3438835 (13.12 MB)\n",
            "Trainable params: 3438835 (13.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUZo87gZGYlz"
      },
      "source": [
        "### IF YOU HAVE MODEL WEIGHT AND WANNA LOAD IT\n",
        "#model.load_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]"
      ],
      "metadata": {
        "id": "elNVIjr_Zl-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTJGHh2hwU96"
      },
      "source": [
        "#define callbacks\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDWU9oCBwU96"
      },
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2zBNnGJwU96"
      },
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJFkP6fwU97"
      },
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV9bR1nlwU97"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvf83C-QwU98"
      },
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zubh3jfwU98",
        "outputId": "62437d43-b860-4089-d901-b635dad382e4"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,10):\n",
        "      model.fit(data, labels, validation_split=0.2, epochs=1, batch_size=128, callbacks=callbacks_list, shuffle=True)\n",
        "      score = model.evaluate(data_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKrBlDrWda-q",
        "outputId": "d47bf93e-f6fa-4390-cbfb-4cbed814cc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 5s 80ms/step - loss: 1.0525 - accuracy: 0.5027 - val_loss: 2.0080 - val_accuracy: 0.0127\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 1.1468 - accuracy: 0.4810\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.8620 - accuracy: 0.6360 - val_loss: 1.9872 - val_accuracy: 0.0618\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0359 - accuracy: 0.5362\n",
            "32/32 [==============================] - 1s 29ms/step - loss: 0.6988 - accuracy: 0.7301 - val_loss: 1.9805 - val_accuracy: 0.1480\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9844 - accuracy: 0.6010\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.5719 - accuracy: 0.7978 - val_loss: 1.9784 - val_accuracy: 0.2098\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9317 - accuracy: 0.6267\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.4740 - accuracy: 0.8336 - val_loss: 2.9475 - val_accuracy: 0.0892\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1491 - accuracy: 0.6133\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.3740 - accuracy: 0.8745 - val_loss: 2.7492 - val_accuracy: 0.1255\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1310 - accuracy: 0.6095\n",
            "32/32 [==============================] - 1s 27ms/step - loss: 0.3149 - accuracy: 0.9066 - val_loss: 2.3685 - val_accuracy: 0.2441\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0824 - accuracy: 0.6295\n",
            "32/32 [==============================] - 1s 31ms/step - loss: 0.2446 - accuracy: 0.9331 - val_loss: 3.0491 - val_accuracy: 0.1941\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 1.2594 - accuracy: 0.6248\n",
            "32/32 [==============================] - 1s 31ms/step - loss: 0.1889 - accuracy: 0.9544 - val_loss: 4.2812 - val_accuracy: 0.0824\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 1.6551 - accuracy: 0.6114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzDo7cKqwU98",
        "outputId": "7d463f5d-e49c-4386-a3b4-dbe55906c62a"
      },
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 0s 10ms/step - loss: 1.6551 - accuracy: 0.6114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE6zEWGNwU99",
        "outputId": "86008b3a-2e22-45fc-9f0a-1adc7c65a410"
      },
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.66\n",
            "accuracy: 61.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6K00ZjfF-9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82b6a48-570f-4a79-db92-8ed7c70887f9"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_SlhL45wU99"
      },
      "source": [
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgukqXFGruD"
      },
      "source": [
        "!cp model.h5 /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}