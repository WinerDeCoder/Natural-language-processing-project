{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sQic7KJRaSP6",
        "outputId": "9e25d309-1cd7-449c-ec9c-e16def63149f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "id": "aoVjFKmOZMZ8",
        "outputId": "e59dffaa-ab34-41f2-e3f4-6e4223f59e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.2.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TFX7wSAmg9y"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab5-CNN+W2V_Sentiment Analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnspI4sLW4JF",
        "outputId": "4efd8b30-2c4a-491a-cd93-a38cda115a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab5-CNN+W2V_Sentiment Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a7lMy03omg93"
      },
      "source": [
        "data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HR1jAzImg94",
        "outputId": "26076c51-0cad-4177-9884-71a8b88e1fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvrbwPfZmg95"
      },
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HlbVeHimg95"
      },
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm4OCwxXmg96"
      },
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2OZgkgmg97"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTb0MeDRmg98"
      },
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW-7mKtWmg9-"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHpPSLTmg9_"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlV3M2dimg9_"
      },
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dl9VZ3Rmg-A",
        "outputId": "31d14639-8e23-455d-9a8a-8f897ea963e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KKSjJdJmg-A"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/my-project/LabNLP/Lab5-CNN+W2V_Sentiment Analysis/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njBANdn5mg-B",
        "outputId": "aeee76e6-5303-4a63-a5ac-af659b4c337b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n",
        "from tensorflow.keras.layers import Reshape, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [2,4,3]\n",
        "num_filters = 128\n",
        "drop = 0.2\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "print(conv_1)\n",
        "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = tf.keras.optimizers.legacy.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 297, 128), dtype=tf.float32, name=None), name='conv1d_16/Relu:0', description=\"created by layer 'conv1d_16'\")\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 300, 400)             3167600   ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)          (None, 299, 128)             102528    ['embedding_1[2][0]']         \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)          (None, 297, 128)             204928    ['embedding_1[2][0]']         \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)          (None, 298, 128)             153728    ['embedding_1[2][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooli  (None, 1, 128)               0         ['conv1d_15[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_16 (MaxPooli  (None, 1, 128)               0         ['conv1d_16[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " max_pooling1d_17 (MaxPooli  (None, 1, 128)               0         ['conv1d_17[0][0]']           \n",
            " ng1D)                                                                                            \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 3, 128)               0         ['max_pooling1d_15[0][0]',    \n",
            " )                                                                   'max_pooling1d_16[0][0]',    \n",
            "                                                                     'max_pooling1d_17[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)         (None, 384)                  0         ['concatenate_5[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 384)                  0         ['flatten_5[0][0]']           \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 3)                    1155      ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3629939 (13.85 MB)\n",
            "Trainable params: 3629939 (13.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0dBlzjmg-D",
        "outputId": "86396164-9bb7-41f7-940d-466aca753fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(data, labels, validation_split=0.2,\n",
        "          epochs=12, batch_size=128, callbacks=callbacks_list, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "32/32 [==============================] - 2s 29ms/step - loss: 7.3590 - accuracy: 0.5213 - val_loss: 6.3687 - val_accuracy: 0.4539\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 5.1494 - accuracy: 0.7721 - val_loss: 6.4383 - val_accuracy: 0.1667\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 4.0167 - accuracy: 0.8672 - val_loss: 5.6400 - val_accuracy: 0.1363\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 3.1900 - accuracy: 0.9191 - val_loss: 5.2399 - val_accuracy: 0.0931\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 2.5430 - accuracy: 0.9458 - val_loss: 4.7439 - val_accuracy: 0.0686\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 2.0305 - accuracy: 0.9662 - val_loss: 4.0602 - val_accuracy: 0.1078\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 1.6442 - accuracy: 0.9728 - val_loss: 3.8106 - val_accuracy: 0.0902\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 1.3393 - accuracy: 0.9757 - val_loss: 3.2151 - val_accuracy: 0.1588\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 1.1152 - accuracy: 0.9760 - val_loss: 3.2227 - val_accuracy: 0.1088\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.9309 - accuracy: 0.9855 - val_loss: 3.2857 - val_accuracy: 0.0735\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.7931 - accuracy: 0.9828 - val_loss: 3.0969 - val_accuracy: 0.0814\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.6939 - accuracy: 0.9821 - val_loss: 2.3903 - val_accuracy: 0.2147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78bd2cb39780>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XoN2UOamg-D"
      },
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwiYb3Ohmg-E"
      },
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E08tBw9img-E"
      },
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgI9Xywmg-E"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p02GxCh6mg-F"
      },
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqUMGInmg-F",
        "outputId": "ff95f28d-c31c-4ae4-f67d-9013e66a93f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKclttiOmg-F",
        "outputId": "3bdf4fcd-3230-4ac5-e3bc-415104ee5e5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 0s 4ms/step - loss: 1.3689 - accuracy: 0.6676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r31_uxxgmg-G",
        "outputId": "84b64bab-754c-4802-95a6-bf47d9980565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 136.89%\n",
            "accuracy: 66.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8O3z4IFmg-G"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}